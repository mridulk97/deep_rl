{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e062aa6-57d1-4688-a1d8-a49ebd9211cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31f2a793-5a60-4c04-b9d6-47ebd68decf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the frozen lake environment using OpenAIâ€™s Gym \n",
    "env = gym.make(\"FrozenLake-v1\") # or the latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c6dd067f-8534-4c3d-b6ae-8d8ad30a27ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# explore the environment\n",
    "observation_space = env.observation_space.n\n",
    "action_space = env.action_space.n\n",
    "\n",
    "print(observation_space)\n",
    "print(action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2b98cd46-7abd-4c0b-8fd5-b8296ef61d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma = 1.0): \n",
    "    \"\"\" \n",
    "    Inputs: \n",
    "        - env: the frozen lake environment. \n",
    "        - gamma: discount factor \n",
    " \n",
    "    Returns: \n",
    "        - value_table: state value function \n",
    "        - Q_value: state-action value function (Q function). \n",
    "    \"\"\"\n",
    "\n",
    "    observation_space = env.observation_space.n\n",
    "    value_table = np.zeros(observation_space)\n",
    "    \n",
    "    threshold = 1e-10\n",
    "    \n",
    "    for i in range(10000):\n",
    "        \n",
    "        prev_value_table = np.copy(value_table)\n",
    "        \n",
    "        for state in range(observation_space):\n",
    "            q_value = []\n",
    "            for action in range(action_space):\n",
    "                next_states_rewards = []\n",
    "                for next_state_reward in env.P[state][action]: \n",
    "                    transition_probability, next_state, reward, done = next_state_reward \n",
    "                    next_states_rewards.append((transition_probability * (reward + gamma * prev_value_table[next_state])))\n",
    "                    \n",
    "                q_value.append(np.sum(next_states_rewards))\n",
    "                \n",
    "            value_table[state] = max(q_value)\n",
    "        \n",
    "        # check for convergence\n",
    "        if (np.sum(np.fabs(prev_value_table - value_table)) <= threshold):\n",
    "             print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "             break\n",
    "    \n",
    "    return value_table, q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "878d40a3-07ed-4321-9f68-f5c312182413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(value_table, gamma = 1.0): \n",
    "    \"\"\" \n",
    "    Inputs: \n",
    "        - value_table: state value function \n",
    "        - gamma: discount factor \n",
    " \n",
    "    Returns: \n",
    "        - policy: the optimal policy. \n",
    "    \"\"\"\n",
    "    policy = np.zeros(observation_space)\n",
    "    \n",
    "    for state in range(observation_space):\n",
    "        q_table = np.zeros(action_space)\n",
    "        \n",
    "        for action in range(action_space):\n",
    "            for next_state_reward in env.P[state][action]:\n",
    "                transition_probability, next_state, reward, done = next_state_reward\n",
    "                q_table[action] += (transition_probability * (reward + gamma * value_table[next_state]))\n",
    "        \n",
    "        # getting argmax\n",
    "        policy[state] = np.argmax(q_table)\n",
    "    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "50f78e49-1c70-4554-ad6f-cf9d9759aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 877.\n"
     ]
    }
   ],
   "source": [
    "optimal_value_function, q_value = value_iteration(env=env, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "556ae547-7d5c-4f0f-96d8-c518e2b9c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = extract_policy(optimal_value_function, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "23d1f51e-99ed-4278-82d9-547880ce11c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 3. 3. 3. 0. 0. 0. 0. 3. 1. 0. 0. 0. 2. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6135fbd0-69e0-4ae1-b6e5-e6468788eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward:  0.838\n"
     ]
    }
   ],
   "source": [
    "all_rewards=[]\n",
    "for _ in range(1000):\n",
    "    obs=env.reset()[0]\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = optimal_policy[obs]\n",
    "        obs,reward,done,info,_ = env.step(action)\n",
    "        if done:\n",
    "            all_rewards.append(reward)\n",
    "            break\n",
    "\n",
    "print(\"Average Reward: \", np.mean(all_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd291f-6133-4659-ac35-a83314231986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
